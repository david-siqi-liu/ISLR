---
title: "Chapter 6 Lab 2"
output: html_notebook
---

```{r}
library(ISLR)

attach(Hitters)

Hitters = na.omit(Hitters)
```

To use `glmnet()` for ridge and lasso, we must:

  - Pass in an `x` matrix
  
  - Pass in a `y` vector
  
  - Do not use the `y ~ x` syntax

```{r}
x = model.matrix(Salary ~ .,
                 Hitters)[, -1]

dim(x)
```

```{r}
y = Hitters$Salary
```

## 6.6.1 - Ridge Regression

`glmnet()` has an `alpha` argument:

  - `alpha = 0`, ridge
  
  - `alpha = 1`, lasso

Note that by default, `glmnet()` **standardizes** the variables. To turn off, set `standardize = FALSE`. This is why we need to make sure there's no missing value in the dataset.

```{r}
library(glmnet)

grid = 10 ^ seq(10,-2, length = 100) # Range of equal distance lambda values from 10^10 to 10^-2

ridge.mod = glmnet(x, y, alpha = 0, lambda = grid)

plot(ridge.mod)
```

`coef()` returns the coefficient grid, with each row corresponding to a predictor and each column corresponding to an $\lambda$.

```{r}
dim(coef(ridge.mod))

coef(ridge.mod)[1:20, 1:5]
```

Let's verify that, when $\lambda$ is large, the coefficient estimates are small.

```{r}
ridge.mod$lambda[50]

coef(ridge.mod)[, 50]
```

Compute the $l_2$ norm.

```{r}
sqrt(sum(coef(ridge.mod)[-1, 50] ^ 2))
```

Let's verify that, when $\lambda$ is small, the coefficient estimates are not small.

```{r}
ridge.mod$lambda[75]

coef(ridge.mod)[, 75]
```

Compute the $l_2$ norm.

```{r}
sqrt(sum(coef(ridge.mod)[-1, 75] ^ 2))
```

We can see that **as $\lambda$ increases, $l_2$ norm decreases**.

We can even use `predict()` to get the coefficient estimates for a new value of $\lambda$.

```{r}
predict(ridge.mod, s = 50, type = 'coefficients')[1:20,]
```

Now, split into training and test sets.

Use a different approach - randomly choose a subset of numbers between 1 and n, then use it as indices for the training observations.

```{r}
set.seed(647)

train = sample(1:nrow(x), nrow(x) / 2) # 50/50 split

length(train)

train[1:20]

test = (-train)

y.test = y[test]
```

Fit the ridge regression model on the training set, with $\lambda$ = 4.

```{r}
ridge.mod = glmnet(x[train, ],
                   y[train],
                   alpha = 0,
                   lambda = grid,
                   thresh = 1e-12)

ridge.pred = predict(ridge.mod, s = 4, newx = x[test, ])

mean((ridge.pred - y.test) ^ 2)
```

Let's compare two things:

  - If we were to fit the model with just an intercept, and
  
  - If we use a very large value of $\lambda$
  
```{r}
mean((mean(y[train]) - y.test) ^ 2)

ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test, ])

mean((ridge.pred - y.test) ^ 2)
```

What about the least square method, which is essentially $\lambda$ = 0?

```{r}
ridge.pred = predict(
  ridge.mod,
  s = 0,
  newx = x[test,],
  exact = TRUE,
  x = x[train,],
  y = y[train]
) # Turn on `exact`, otherwise it would interpolate over the grid

mean((ridge.pred - y.test) ^ 2)

print("Least square:")

lm(y ~ x, subset = train)

print("Ridge: ")

predict(
  ridge.mod,
  s = 0,
  exact = TRUE,
  type = "coefficients",
  x = x[train,],
  y = y[train]
)[1:20,]
```

We can use cv to choose $\lambda$. There's a built-in `cv.glmnet()` function. By default, `k` = 10.

```{r}
set.seed(647)

cv.out = cv.glmnet(x[train,],
                   y[train],
                   alpha = 0)

plot(cv.out)
```

```{r}
bestlam = cv.out$lambda.min

bestlam
```

The MSE.

```{r}
ridge.pred = predict(ridge.mod,
                     s = bestlam,
                     newx = x[test,])

mean((ridge.pred - y.test) ^ 2)
```

Finally, refit the model on the full data set, using the $\lambda$ chosen by cv.

```{r}
out = glmnet(x, y, alpha = 0)

predict(out, type = "coefficients", s = bestlam)[1:20,]
```

## 6.6.2 - Lasso

Now, use `alpha` = 1 for lasso regression.

```{r}
lasso.mod = glmnet(x[train,],
                   y[train],
                   alpha = 1,
                   lambda = grid)

plot(lasso.mod)
```

Let's verify that, when $\lambda$ is large, some coefficients become zero.

```{r}
lasso.mod$lambda[75]

coef(lasso.mod)[,75]
```

Compute the $l_2$ norm.

```{r}
sqrt(sum(abs(coef(lasso.mod)[-1, 75])))
```

Let's verify that, when $\lambda$ is small, most coefficients are not zero.

```{r}
lasso.mod$lambda[80]

coef(lasso.mod)[,80]
```

Compute the $l_1$ norm.

```{r}
sqrt(sum(abs(coef(lasso.mod)[-1, 80])))
```

We can see that **as $\lambda$ increases, $l_1$ norm also decreases (not always, but in general)**.

Now, let's do cv on $\lambda$.

```{r}
set.seed(647)

cv.out = cv.glmnet(x[train,],
                   y[train],
                   alpha = 1)

plot(cv.out)
```

```{r}
bestlam = cv.out$lambda.min

bestlam
```

```{r}
lasso.pred = predict(lasso.mod,
                     s = bestlam,
                     newx = x[test,])

mean((lasso.pred - y.test) ^ 2)
```

Final model.

```{r}
out = glmnet(x, y, alpha = 1, lambda = grid)

lasso.coef = predict(out, type = 'coefficients', s = bestlam)[1:20, ]

lasso.coef
```

Doesn't look like any coefficient has been set to zero.